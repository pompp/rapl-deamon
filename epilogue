#!/bin/bash
# Job Prologue/Epilogue Scripts
#
# This script will be executed by owner of "slurmctld" process.
# It is defined by "SlurmUser" parameter in slurm.conf
#
# Environment Variables in Prologue and Epilogue
#   When invoked, the prologue is called with the following values:
#	SLURM_JOB_NAME          # sbatch -J xxxx
#	SLURM_JOB_GROUP         # groupname of owner
#	SLURM_JOBID             # JOB ID
#	SLURM_JOB_ID            # JOB ID
#	SLURM_JOB_USER          # username of owner
#	SLURM_JOB_UID           # user ID  of owner
#	SLURM_JOB_NODELIST      # hostlist in SLURM's expression format
#	SLURM_JOB_GID           # group ID of owner
#	SLURM_CLUSTER_NAME      # ClusterName (slurm.conf)
#	SLURM_JOB_PARTITION     # PartitionName (slurm.conf)
#	SLURM_JOB_EXIT_CODE	# exit code of job (only Epilogue)
#
SLURM_ROOT=/usr/local
RAPLC_ROOT=/usr/local
SLURM_BIN=${SLURM_ROOT}/bin
RAPLC_BIN=${RAPLC_ROOT}/bin
HOSTLISTFILE=/tmp/slurm_hostlist.$$

export PATH=${SLURM_BIN}:${RAPLC_BIN}:/bin:/usr/sbin:/usr/bin
export TZ=JST-9

if [ "$SLURM_JOB_ID" = "" ]; then
	exit 0
fi
JOBPROF=/tmp/job_prof.${SLURM_JOB_ID}
touch $JOBPROF
chmod 600 $JOBPROF
if [ ! -f $JOBPROF ]; then
	exit 0
fi

if [ "$SLURM_JOB_NODELIST" = "" ]; then
	exit 0
fi

if [ ! -x ${SLURM_BIN}/scontrol ]; then
	exit 0
fi

touch $HOSTLISTFILE
chmod 600 $HOSTLISTFILE
if [ ! -f $HOSTLISTFILE ]; then
	rm -f $JOBPROF
	exit 0
fi

scontrol show hostnames $SLURM_JOB_NODELIST > $HOSTLISTFILE 2> /dev/null

if [ ! -s $HOSTLISTFILE ]; then
	rm -f $HOSTLISTFILE
	exit 0
fi

for HOST in `cat $HOSTLISTFILE`; do
	raplc -r -i 1000 -t -f ondemand $HOST 2> /dev/null >> $JOBPROF
done

rm -f $HOSTLISTFILE

exit 0
